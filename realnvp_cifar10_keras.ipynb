{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T14:58:36.063138Z",
     "start_time": "2019-10-11T14:58:35.057842Z"
    },
    "papermill": {
     "duration": 1.041988,
     "end_time": "2019-10-11T20:38:01.653237",
     "exception": false,
     "start_time": "2019-10-11T20:38:00.611249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_probability.python.bijectors as tfb\n",
    "import tensorflow_probability.python.distributions as tfd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "devices = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for d in devices:\n",
    "    tf.config.experimental.set_memory_growth(d, True)\n",
    "\n",
    "print(devices)\n",
    "    \n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "# tf.debugging.enable_check_numerics()\n",
    "\n",
    "# Disable LayoutOptimizer since it raises reshape error (Why?)\n",
    "tf.config.optimizer.set_experimental_options({'layout_optimizer': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = pathlib.Path('./logs/realnvp_cifar10_keras_test_run')\n",
    "ckpt_dir = log_dir / 'checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002914,
     "end_time": "2019-10-11T20:38:01.659227",
     "exception": false,
     "start_time": "2019-10-11T20:38:01.656313",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_info = tfds.load(\n",
    "        'cifar10',\n",
    "        split=None, #tfds.Split.ALL,\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=True,\n",
    ")\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return dataset.map(\n",
    "        lambda img, label: (\n",
    "            0.5 + 0.5 * tf.cast(img, tf.float32) / 256.0,\n",
    "            label,\n",
    "        )\n",
    "    )\n",
    "\n",
    "dataset['train'] = preprocess(dataset['train'])\n",
    "dataset['test'] = preprocess(dataset['test'])\n",
    "\n",
    "h, w, c = dataset_info.features['image'].shape\n",
    "num_train_examples = dataset_info.splits['train'].num_examples\n",
    "num_test_examples = dataset_info.splits['test'].num_examples\n",
    "\n",
    "sample = next(iter(dataset['train']))\n",
    "plt.imshow(sample[0].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003731,
     "end_time": "2019-10-11T20:38:01.790099",
     "exception": false,
     "start_time": "2019-10-11T20:38:01.786368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.166156,
     "end_time": "2019-10-11T20:38:01.959682",
     "exception": false,
     "start_time": "2019-10-11T20:38:01.793526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShiftAndLogScale(tf.Module):\n",
    "    def __init__(self, output_units, name='shift_and_log_scale'):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.output_units = output_units\n",
    "        \n",
    "        self.net = K.Sequential([\n",
    "            K.layers.BatchNormalization(),\n",
    "            K.layers.Conv2D(256, 3, 1, 'same', activation='relu'),\n",
    "            K.layers.BatchNormalization(),\n",
    "            K.layers.Conv2D(512, 3, 1, 'same', activation='relu'),\n",
    "            K.layers.BatchNormalization(),\n",
    "            K.layers.Conv2D(output_units*2, 3, 1, 'same', activation=None),\n",
    "        ])\n",
    "        \n",
    "    @tf.function\n",
    "    def __call__(self, x, output_units):\n",
    "        assert output_units == self.output_units\n",
    "        x = self.net(x)\n",
    "        shift, log_scale = tf.split(x, 2, axis=-1)\n",
    "        return shift, log_scale\n",
    "\n",
    "\n",
    "class RealNVP(tfb.Chain):\n",
    "    def __init__(self, n_layers, n_masked, n_units, name=None):\n",
    "        def make_layer(i):\n",
    "            fn = ShiftAndLogScale(n_units - n_masked)\n",
    "            chain = [\n",
    "                tfb.RealNVP(\n",
    "                    num_masked=n_masked,\n",
    "                    shift_and_log_scale_fn=fn,\n",
    "                ),\n",
    "                tfb.BatchNormalization(),\n",
    "            ]\n",
    "            if i % 2 == 0:\n",
    "                perm = lambda: tfb.Permute(permutation=[2, 0, 1], axis=-1)\n",
    "                chain = [perm(), *chain, perm()]\n",
    "            return tfb.Chain(chain)\n",
    "\n",
    "        chain = [\n",
    "            tfb.Sigmoid(),\n",
    "            *[make_layer(i) for i in range(n_layers)],\n",
    "            tfb.Reshape((h, w, 3), (-1,)),\n",
    "        ]\n",
    "        super().__init__(chain, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper classes for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedDistribution(tfd.TransformedDistribution):\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.variables\n",
    "\n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return self.trainable_variables\n",
    "\n",
    "    @property\n",
    "    def non_trainable_weights(self):\n",
    "        return tuple(filter(lambda v: not getattr(v, 'trainable', False), self.weights))\n",
    "    \n",
    "class LogProb(K.Model):\n",
    "    def __init__(self, distribution, bijector):\n",
    "        super().__init__()\n",
    "        self.flow = TransformedDistribution(\n",
    "            distribution=distribution,\n",
    "            bijector=bijector,\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        log_prob = self.flow.log_prob(x)\n",
    "        return log_prob\n",
    "\n",
    "class NegativeLogLikelihood(K.losses.Loss):\n",
    "    def call(self, _, log_prob):\n",
    "        return -log_prob / (h*w*c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = tfd.MultivariateNormalDiag(\n",
    "    loc=tf.zeros(h*w*3, name='loc'),\n",
    "    scale_diag=tf.ones(h*w*3, name='scale_diag'),\n",
    "    name='distribution',\n",
    ")\n",
    "\n",
    "bijector = RealNVP(n_layers=10, n_masked=2, n_units=3, name='bijector') \n",
    "\n",
    "model = LogProb(distribution, bijector)\n",
    "loss_fn = NegativeLogLikelihood(name='nll')\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=1e-3),\n",
    "    loss=loss_fn,\n",
    ")\n",
    "model.build((None, h, w, c))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sampling_callback(flow, steps_per_epoch, n_samples=9):\n",
    "    def calibrate(samples):\n",
    "        samples = (samples - 0.5) / 0.5\n",
    "        samples = tf.clip_by_value(samples, 0.0, 1.0)\n",
    "        return samples\n",
    "\n",
    "    normal_samples = flow.distribution.sample(n_samples)\n",
    "    def _(epoch, logs):\n",
    "        print(epoch)\n",
    "        samples = flow.bijector.forward(normal_samples)\n",
    "        samples = calibrate(samples)\n",
    "\n",
    "        tf.summary.image(\n",
    "            'samples', samples,\n",
    "            step=epoch*steps_per_epoch,\n",
    "            max_outputs=n_samples\n",
    "        )\n",
    "    return _\n",
    "        \n",
    "callbacks = [\n",
    "    K.callbacks.TensorBoard(\n",
    "        log_dir=log_dir.as_posix(), update_freq='batch',\n",
    "        histogram_freq=100,\n",
    "        write_graph=True, write_images=True,\n",
    "    ),\n",
    "    K.callbacks.ModelCheckpoint(\n",
    "        ckpt_dir.as_posix(), save_weights_only=True, verbose=1,\n",
    "    ),\n",
    "    K.callbacks.LambdaCallback(\n",
    "        on_epoch_end=sampling_callback(\n",
    "            model.flow, num_train_examples),\n",
    "    ),\n",
    "]\n",
    "\n",
    "batchsize = 64\n",
    "\n",
    "history = model.fit(\n",
    "    dataset['train'].batch(batchsize).repeat(),\n",
    "    steps_per_epoch=num_train_examples,\n",
    "    epochs=100,\n",
    "    validation_data=dataset['test'].batch(batchsize).repeat(),\n",
    "    validation_steps=num_test_examples,\n",
    "    callbacks=callbacks,\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "papermill": {
   "duration": 388.408254,
   "end_time": "2019-10-11T20:44:27.118921",
   "environment_variables": {},
   "exception": null,
   "input_path": "digits_2d.ipynb",
   "output_path": "realnvp_digits_2d.ipynb",
   "parameters": {},
   "start_time": "2019-10-11T20:37:58.710667",
   "version": "1.2.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
